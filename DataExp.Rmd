---
title: "Data Exploration"
output:
  pdf_document: default
  html_document: default
date: '2022-06-28'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages

```{r, echo=FALSE}
library(tidyverse)
library(brms)
library(tidybayes)

```

## Loading data

```{r and cleaning data}
data <- read_csv('turf_prod_val.csv') |> 
  filter(!is.na(depth))

unit <- names(data)[1]
names(data)[1] <- 'prod'

x <- str_split(data$prod, '\xb1')
data$mean_prod <- as.numeric(substr(unlist(lapply(x, function(x)x[1])),1,4))
data$se_prod <- as.numeric(substr(unlist(lapply(x, function(x)x[2])),2,5))

data <- data %>% filter(mean_prod != 0)

```

## Now, for the data points we do not have se, determine one from the relationship between mean and se:

```{r , echo=FALSE}

ggplot(data %>% filter(!is.na(se_prod))) +
  geom_point(aes(x=mean_prod,y=se_prod))

```
## Predicting variability using the mean for 14 points and also adjusting McClure 2019,
## which is a ci and not se, and also adding a small non zero value to all zero se

```{r predicting se}
mod_se <- lm(se_prod ~ mean_prod, data=data)

data[is.na(data$se_prod),'se_prod'] <- round(predict(mod_se, newdata=data[is.na(data$se_prod),]),2)

data[data$Ref == 'McClure 2019', 'se_prod'] <- data[data$Ref == 'McClure 2019', 'se_prod'] / 1.96

nzmin <- function(x) min(x[x>0])

data[data$se_prod == 0,'se_prod'] <- nzmin(data$se_prod)

```



## And finally modelling

```{r, echo=FALSE}


pri <- get_prior(mean_prod | se(se_prod, sigma=TRUE) ~ 1 + log(depth),
          data = data,
          family=skew_normal())

brmod <- brm(mean_prod | se(se_prod, sigma=TRUE) ~ 1 + log(depth), 
             data = data,
             family=skew_normal(),
             prior = pri,
             chains = 4, iter = 5000, thin = 3)

```

## Loading and tidying data to predict for

```{r}

## Constrained max depth of site to 15m

pred_depth <- read.csv('moorea_depth.csv') %>% 
  mutate(site=tolower(site)) %>%
  group_by(site) %>%
  mutate(depth=if_else(depth < -15,-15, depth)*-1) %>%
  slice_max(depth)

pred_data <- read.csv('moorea_benthos.csv') %>% 
  mutate(site=gsub('\\s','_',tolower(site)))


```


## Filtering and manipulating the time series for the correct categories

```{r}

ts_data <- left_join(pred_data,pred_depth, by='site') %>%
  filter(Habitat=='Outer slope' & Season=='Mar') %>% 
  mutate(subs_group=case_when(
      Substrate == 'Dead coral' ~ 'algal_turf',
      Substrate == 'Stegastes Turf' ~ 'algal_turf',
      Substrate == 'Rubble' ~ 'algal_turf',
      Substrate == 'Pavement' ~ 'algal_turf',
      Substrate == 'Macroalgae' ~ 'macroalgae',
      Substrate == 'Turbinaria' ~ 'macroalgae',
      TRUE ~ Substrate,
  )) %>%
  filter(subs_group %in% c('algal_turf','macroalgae')) %>%
  group_by(Year, site, Transect, lat, long, depth, subs_group) %>%
  summarise(prop=sum(proportion), .groups='drop_last') %>%
  pivot_wider(names_from=subs_group,values_from=prop,values_fill=0)


```


## Now, how about trying to predict benthic reef productivity
## by merging area specific turf productivity with turf cover

```{r}

pred_val <- posterior_epred(brmod, 
      newdata=ts_data %>% mutate(se_prod=0.1), ndraws=1000)


dim(ts_data)
dim(pred_val)

tot_turf_prod <- (ts_data$algal_turf * t(pred_val)) * 10000
## in g C ha day-1


# aggregating

fts_data <- cbind(ts_data,
      turf_prod_mean=apply(tot_turf_prod,1,median),
      turf_prod_lhdi=apply(tot_turf_prod,1,function(x) median_hdci(x)$ymin),
      turf_prod_uhdi=apply(tot_turf_prod,1,function(x) median_hdci(x)$ymax))

## in g C ha-1 day-1
```

## Now doing some plots

```{r}

ggplot(data=fts_data) +
  geom_line(aes(x=Year,y=turf_prod_mean,group=interaction(Transect,site))) +
  geom_smooth(aes(x=Year,y=turf_prod_mean), colour='orange', alpha=0.8) +
  scale_x_continuous(breaks=c(2005,2009,2013,2017)) +
  ylab(expression(Algal~turf~productivity~'('*g~C~ha^-1*day^-1*')')) +
  theme_minimal()

```



